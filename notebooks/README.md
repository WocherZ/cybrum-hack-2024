# Ноутбуки с экспериментами

## Бозовое тестирование моделей
**_TestModelsFromTransformers.ipynb_**

Этот ноутбук демонстрирует процесс тестирования и генерации текста с использованием различных языковых моделей. В нем использована библиотека transformers для загрузки моделей и токенизаторов из Hugging Face Hub. Код ориентирован на использование моделей, таких как Llama, Mistral и другие, с возможностью генерации текста на основе заданных подсказок.

В этом ноутбуке используется ряд моделей из библиотеки Hugging Face. Некоторые из них включают:

- Vikhr Llama 3.2 1B
- Vikhr Gemma2B Instruct
- Vikhr Qwen 2.5 0.5B
- Vikhr Nemo 12B
- RuGPT3 Large Based on GPT-2
- Qwen 2.5 05B Instruct
- Qwen 2.5 1.5B
- mGPT
- Llama 3.2 3B Instruct
- Phi 3.5 mini instruct
- AYA 101
- mT0 xxl
- mT0 large
- Bloomz-7B1 mt
- Bloomz-7B1
- Mistral 7B v0.1
- Mistral Nemo Base

Ноутбук был сделан с целью выявления особенностей ответа каждой из моделей этого списка и сравнения их ответов по полноте и качеству.

## Тестирование Llama.cpp
**_Llama CPP Tutorial.ipynb_**

Этот ноутбук содержит примеры взаимодействия с библиотекой `llama.cpp` из `Python`. Ноутбук был позаимствован из обучающего [видео](https://youtu.be/0SK6H9Vmw6M?si=Ag4Z0vI8USCyVMMp).


## Генерация датасета

**_Dataset_generation.ipynb_**

Генерация датасета производилась с помощью более сильных языковых моделей (пример для [Qwen2.5-72B](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)).
Используя различные инструкции по генерации текстов определялись ограничения на генерацию диалогов моделью.
Тем самым удалось достичь достаточно разнообразных диалогов как по содержанию, так и набору различных инструкций в датасете.

## Дообучение моделей

**_Fine-tuning_turbo_alignment.ipynb_**

Дообучение моделей производилось с помощью фреймворка [turbo-alignment](https://github.com/turbo-llm/turbo-alignment).
Перед обучением проводилась установка всех необходимых зависимостей и проверка базовой модели на тестовом датасете.
Далее производилось обучение с помощью метода [Low-Rank Adaptation](https://arxiv.org/abs/2106.09685).
Данный метод был выбран по следующим причинам:
- Подходит для обучения на небольшом кол-ве данных
- Не требует большого количества вычислительных ресурсов, так как обучается несколько процентов от всех параметров модели
- Не увеличивает время генерации ответа модели после дообучения

После дообучения производился инференс модели и проверка на тестовом датасете. После ответы вручную оценивались человеком.