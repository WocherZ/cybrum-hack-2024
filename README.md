# Хакатон Цифрум

Репозиторий решения команды AXIOM на хакатоне по предиктивной аналитике Цифрум.

# Задача

Постановка задачи: Разработать модель, которая способна генерировать текст на основе предоставленного начального текста и поставленной бизнес задачи.

Решение было сфокусированно на генерации маркетинговых текстов для размещения рекламы продуктов Росатома на различных площадках. Это особенно важно, так как помогает ускорить процесс создания контента и повысить разнообразие без необходимости значительных временных затрат.

# Команда: AXIOM
- Зворыгин Владимир Андреевич
- Миронов Андрей Михайлович
- Диков Александр Евгеньевич
- Садохин Алексей Александрович

# Структура проекта
- **_backend_**: fast api + развёртывание моделей с помощью [llama.cpp](https://github.com/ggerganov/llama.cpp)
- **_data_**: датасеты для обучения и валидации моделей
- **_frontend_**: [streamlit](https://streamlit.io/) приложение, позволяющее початиться с моделью
- **_models_**: модели, которые использовались для экспериментов и обучения
- **_notebooks_**: ноутбуки с экспериментами по задаче и тестированием разных моделей

# Идея решения

Для генерации текстов хорошо подходят большие языковые модели. Однако использование коммерческих моделей (напр. ChatGPT, GigaChat или Yandex.GPT) может быть недопустимо, так как для генерации текстов может использоваться конфиденциальная инфомрация. 
Более того коммерческие модели не обладают знаниями о промышленной области и в частности специфике Росатома. Поэтому решение было заточенно под использование open-source моделей.

В качестве базовой модели были рассмотрены несколько вариантов. Основными критериями при выборе были:

- Небольшой размер модели для возможности дообучения на собственных датасетах 
- Достаточное качество генерации текстов на русском языке

После выбора базовой модели производилось дообучение с помощью метода [Low-Rank Adaptation](https://arxiv.org/abs/2106.09685). 
Данный метод был выбран как эффективный метод дообучения больших языковых моделей под конкретную узкоспециализированную задачу.

Обучающие и тестовые датасеты были сгененрированы с помощью моделей gpt-3.5-turbo и [Qwen2.5-72B](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct). 
После генерации множества обучающих примеров проводилась фильтрация датасета на основе семантического сходства примеров.
Датасеты составлены таким образом, чтобы учитывать специфику промышленной области и цифровых продуктов Росатома.

После дообучения базовых моделей качество проверялось на валидационном датасете, а также отдельными запросами, для проверки общих способностей к генерации рекламных текстов на основе входных инструкций. 

# Демо решения

# Разворачивание решения
