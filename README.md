# Хакатон Цифрум

Репозиторий решения команды AXIOM на хакатоне по предиктивной аналитике Цифрум.

# Задача

Постановка задачи: Разработать модель, которая способна генерировать текст на основе предоставленного начального текста и поставленной бизнес задачи.

Решение было сфокусированно на генерации маркетинговых текстов для размещения рекламы продуктов Росатома на различных площадках. Это особенно важно, так как помогает ускорить процесс создания контента и повысить разнообразие без необходимости значительных временных затрат.

# Команда: AXIOM
- Зворыгин Владимир Андреевич
- Миронов Андрей Михайлович
- Диков Александр Евгеньевич
- Садохин Алексей Александрович

# Структура проекта
- **_backend_**: fast api + развёртывание моделей с помощью [llama.cpp](https://github.com/ggerganov/llama.cpp)
- **_data_**: датасеты для обучения и валидации моделей
- **_frontend_**: [streamlit](https://streamlit.io/) приложение, позволяющее початиться с моделью
- **_models_**: модели, которые использовались для экспериментов и обучения
- **_notebooks_**: ноутбуки с экспериментами по задаче и тестированием разных моделей

# Идея решения

Для генерации текстов хорошо подходят большие языковые модели. Однако использование коммерческих моделей (напр. ChatGPT, GigaChat или Yandex.GPT) может быть недопустимо, так как для генерации текстов может использоваться конфиденциальная инфомрация. 
Более того коммерческие модели не обладают знаниями о промышленной области и в частности специфике Росатома. Поэтому решение было заточенно под использование open-source моделей.

В качестве базовой модели были рассмотрены несколько вариантов. Основными критериями при выборе были:

- Небольшой размер модели для возможности дообучения на собственных датасетах 
- Достаточное качество генерации текстов на русском языке

После выбора базовой модели производилось дообучение с помощью метода [Low-Rank Adaptation](https://arxiv.org/abs/2106.09685). 
Данный метод был выбран как эффективный метод дообучения больших языковых моделей под конкретную узкоспециализированную задачу.

Обучающие и тестовые датасеты были сгененрированы с помощью моделей gpt-3.5-turbo и [Qwen2.5-72B](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct). 
После генерации множества обучающих примеров проводилась фильтрация датасета на основе семантического сходства примеров.
Датасеты составлены таким образом, чтобы учитывать специфику промышленной области и цифровых продуктов Росатома.

После дообучения базовых моделей качество проверялось на валидационном датасете, а также отдельными запросами, для проверки общих способностей к генерации рекламных текстов на основе входных инструкций. 
Таким образом удалось достичь хорошего качества генерации маркетиноговых текстов по промышленной тематике на небольших языковых моделях, которые можно запустить локально даже на cpu, предварительно сконвертировав в gguf формат.

# Демо решения

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/fzjcR0PNTic/0.jpg)](https://www.youtube.com/watch?v=fzjcR0PNTic)

[Также демо есть на RuTube](https://rutube.ru/video/7bf6ec76187f60524a52f55b2a1b2225/)

# Разворачивание решения

### **Шаг 1: Клонирование репозитория или подготовка проекта**

Клонируйте репозитории Git:

```commandline
git clone https://github.com/WocherZ/cybrum-hack-2024.git
cd cybrum-hack-2024
```

### **Шаг 2: Создание виртуального окружения**

Рекомендуется использовать виртуальное окружение для изоляции зависимостей проекта.

```commandline
python3 -m venv venv
```

Активируйте виртуальное окружение:

- На Windows:
    ```commandline
    venv\Scripts\activate
    ```

- На macOS и Linux:
    ```commandline
    source venv/bin/activate
    ```

### **Шаг 3: Установка зависимостей из requirements.txt**

Убедитесь, что файл requirements.txt находится в корне вашего проекта. Для установки зависимостей выполните:

```commandline
pip install --upgrade pip
pip install -r requirements.txt
```

Обратите внимание, что requirements.txt не содержит всех зависимостей, необходимых в ноутбуках из папки [notebooks](/notebooks/)

Это установит все необходимые библиотеки.

### **Шаг 4: Запуск Streamlit приложения**

После установки зависимостей и моделей запустите ваше Streamlit приложение, перейдя в папку [frontend](/frontend/):

```commandline
streamlit run main.py
```

После выполнения команды, Streamlit запустит локальный сервер, и в командной строке отобразится URL (обычно http://localhost:8501), по которому можно открыть приложение в браузере.

### **Шаг 6: Запуск Backend**

Запустите бэкенд приложение на FastAPI, перейдя в папку [backend](/backend/):

```commandline
uvicorn main:app --reload
```

После выполнения команды запустится веб-сервер. Далее можно переходить к интерфейсу на Streamlit http://localhost:8501 и пользоваться разработанным приложением.
